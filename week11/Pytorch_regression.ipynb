{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boolean-helmet",
   "metadata": {},
   "source": [
    "# Predicting house prices with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sunset-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-interaction",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-spider",
   "metadata": {},
   "source": [
    "### loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grave-snake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"data/train.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjacent-substance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-officer",
   "metadata": {},
   "source": [
    "### Extracting the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identified-wildlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "MSSubClass         int64\n",
       "MSZoning          object\n",
       "LotFrontage      float64\n",
       "LotArea            int64\n",
       "                  ...   \n",
       "MoSold             int64\n",
       "YrSold             int64\n",
       "SaleType          object\n",
       "SaleCondition     object\n",
       "SalePrice          int64\n",
       "Length: 81, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-corrections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice'] \n",
      " 38\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = list(raw_data.columns[(raw_data.dtypes==np.int64) |\n",
    "                 (raw_data.dtypes==np.float64)])\n",
    "print(numeric_columns, \"\\n\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-incidence",
   "metadata": {},
   "source": [
    "Set `SalesPrice` as the last index, since it is the value we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intermediate-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns.remove('SalePrice')\n",
    "numeric_columns.append('SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-attribute",
   "metadata": {},
   "source": [
    "We do not need the `Id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "marked-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns.remove('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-vertex",
   "metadata": {},
   "source": [
    "Now we extract the numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "irish-probe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  WoodDeckSF  \\\n",
       "0          2003       196.0         706           0  ...           0   \n",
       "1          1976         0.0         978           0  ...         298   \n",
       "2          2002       162.0         486           0  ...           0   \n",
       "3          1970         0.0         216           0  ...           0   \n",
       "4          2000       350.0         655           0  ...         192   \n",
       "\n",
       "   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "0           61              0          0            0         0        0   \n",
       "1            0              0          0            0         0        0   \n",
       "2           42              0          0            0         0        0   \n",
       "3           35            272          0            0         0        0   \n",
       "4           84              0          0            0         0        0   \n",
       "\n",
       "   MoSold  YrSold  SalePrice  \n",
       "0       2    2008     208500  \n",
       "1       5    2007     181500  \n",
       "2       9    2008     223500  \n",
       "3       2    2006     140000  \n",
       "4      12    2008     250000  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_data = raw_data[numeric_columns]\n",
    "numeric_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-cartoon",
   "metadata": {},
   "source": [
    "Now let's deal with the missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "obvious-rider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'MasVnrArea', 'GarageYrBlt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_columns = np.any(pd.isna(numeric_data), axis = 0)\n",
    "nan_columns = list(nan_columns[nan_columns == True].index)\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-center",
   "metadata": {},
   "source": [
    "We simply replace them with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quarterly-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/ahmad/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ahmad/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "numeric_data['LotFrontage'] = numeric_data['LotFrontage'].fillna(0)\n",
    "numeric_data['MasVnrArea'] = numeric_data['MasVnrArea'].fillna(0)\n",
    "numeric_data['GarageYrBlt'] = numeric_data['GarageYrBlt'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-grade",
   "metadata": {},
   "source": [
    "let's split the data for training and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "northern-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "numeric_data_train, numeric_data_test = train_test_split(numeric_data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-candy",
   "metadata": {},
   "source": [
    "### Normalizing the data\n",
    "Before training our linear regression model, we have to normalize the data. We do this by subtracting each column from its minimum value and then dividing it by the difference between maximum and minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dominant-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving max, min for each column\n",
    "maxs, mins = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pursuant-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_data:\n",
    "    maxs[col] = numeric_data_train[col].max()\n",
    "    mins[col] = numeric_data_train[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "resistant-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data_train = (numeric_data_train - numeric_data_train.min()) / (numeric_data_train.max() - numeric_data_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-notion",
   "metadata": {},
   "source": [
    "## Building a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "insured-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "perfect-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_x_columns = list(numeric_data_train.columns)\n",
    "numeric_x_columns.remove(\"SalePrice\")\n",
    "X_train_df = numeric_data_train[numeric_x_columns]\n",
    "y_train_df = pd.DataFrame(numeric_data_train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-surge",
   "metadata": {},
   "source": [
    "Now we have to convert the data into torch tensors. A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type. It's very similar to arrays in `NumPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ranging-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train_df.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train_df.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prescribed-reason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1314, 36]) torch.Size([1314, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.size(), y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-jamaica",
   "metadata": {},
   "source": [
    "### Defining a model with pytorch\n",
    "A model is always defined as a class in pytorch. It should have a `__init__` function in which you define the layers of your network. It also should have a `forward` function (method) that basically defines the forward pass on the network.\n",
    "\n",
    "For the beggining, let's start with a single layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "written-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, D_out)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))\n",
    "        y_pred = self.linear2(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sustainable-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, D_out = X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "subjective-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the first model: an instance of the class \"Net\"\n",
    "model1 = Net(D_in, 500, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-flash",
   "metadata": {},
   "source": [
    "The next steps is to define the __loss criterion__ and the __optimizer__ for the network. That is, we have to define the loss function we want to optimize during training and also the optimization method we are going to use, e.g, SGD, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "permanent-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-gentleman",
   "metadata": {},
   "source": [
    "Now, we are ready to do the training. We can simply do this by a for loop over the number of iterations. The training has 3 main steps:\n",
    "- A forward pass to compute the prediction for the current data point (batch).\n",
    "- computing the loss for the current prediction.\n",
    "- A backward pass to compute the gradient of the loss with respect to the weight of the network.\n",
    "- Finaly, updating the weights of the network (`optimizer.step()`).\n",
    "\n",
    "Note that in each backward pass pytorch saves the gradient for all of the parameters. Therefore it is important to replace the old gradient values with zero in the beggining of each iteration, otherwise the gradients will be accumulated during the iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "earlier-maximum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 212.12074279785156\n",
      "1 1904.125\n",
      "2 13151.103515625\n",
      "3 1848.936279296875\n",
      "4 133.611572265625\n",
      "5 74.94303131103516\n",
      "6 49.6674690246582\n",
      "7 37.99897766113281\n",
      "8 32.24648666381836\n",
      "9 29.167255401611328\n",
      "10 27.332897186279297\n",
      "11 26.08904266357422\n",
      "12 25.13568687438965\n",
      "13 24.330886840820312\n",
      "14 23.608631134033203\n",
      "15 22.93484115600586\n",
      "16 22.296741485595703\n",
      "17 21.68573570251465\n",
      "18 21.097606658935547\n",
      "19 20.52947998046875\n",
      "20 19.97885513305664\n",
      "21 19.446849822998047\n",
      "22 18.932147979736328\n",
      "23 18.434091567993164\n",
      "24 17.950437545776367\n",
      "25 17.481311798095703\n",
      "26 17.026901245117188\n",
      "27 16.586393356323242\n",
      "28 16.159038543701172\n",
      "29 15.744958877563477\n",
      "30 15.343223571777344\n",
      "31 14.954510688781738\n",
      "32 14.578536033630371\n",
      "33 14.214092254638672\n",
      "34 13.861385345458984\n",
      "35 13.519657135009766\n",
      "36 13.188273429870605\n",
      "37 12.867388725280762\n",
      "38 12.557364463806152\n",
      "39 12.25770378112793\n",
      "40 11.968367576599121\n",
      "41 11.689340591430664\n",
      "42 11.42078971862793\n",
      "43 11.16317367553711\n",
      "44 10.915022850036621\n",
      "45 10.676183700561523\n",
      "46 10.446859359741211\n",
      "47 10.226234436035156\n",
      "48 10.014737129211426\n",
      "49 9.810474395751953\n",
      "50 9.614521026611328\n",
      "51 9.427712440490723\n",
      "52 9.248559951782227\n",
      "53 9.077019691467285\n",
      "54 8.912934303283691\n",
      "55 8.755955696105957\n",
      "56 8.60519027709961\n",
      "57 8.46041488647461\n",
      "58 8.320914268493652\n",
      "59 8.186749458312988\n",
      "60 8.058621406555176\n",
      "61 7.936429500579834\n",
      "62 7.819355487823486\n",
      "63 7.70740270614624\n",
      "64 7.600005149841309\n",
      "65 7.497235298156738\n",
      "66 7.399192810058594\n",
      "67 7.305692195892334\n",
      "68 7.2165207862854\n",
      "69 7.1313395500183105\n",
      "70 7.0497026443481445\n",
      "71 6.971477031707764\n",
      "72 6.8964385986328125\n",
      "73 6.824830532073975\n",
      "74 6.756499767303467\n",
      "75 6.691046714782715\n",
      "76 6.628307342529297\n",
      "77 6.568062782287598\n",
      "78 6.510441303253174\n",
      "79 6.455131530761719\n",
      "80 6.401978015899658\n",
      "81 6.350834846496582\n",
      "82 6.30172872543335\n",
      "83 6.254180908203125\n",
      "84 6.208475589752197\n",
      "85 6.164466857910156\n",
      "86 6.122034072875977\n",
      "87 6.080856800079346\n",
      "88 6.040987968444824\n",
      "89 6.002511978149414\n",
      "90 5.96522855758667\n",
      "91 5.929233074188232\n",
      "92 5.8943939208984375\n",
      "93 5.8606276512146\n",
      "94 5.827986717224121\n",
      "95 5.796396255493164\n",
      "96 5.765783786773682\n",
      "97 5.7360076904296875\n",
      "98 5.7069411277771\n",
      "99 5.678589820861816\n",
      "100 5.650949954986572\n",
      "101 5.6239333152771\n",
      "102 5.59759521484375\n",
      "103 5.5718536376953125\n",
      "104 5.546653747558594\n",
      "105 5.522039413452148\n",
      "106 5.497971057891846\n",
      "107 5.474486827850342\n",
      "108 5.451540946960449\n",
      "109 5.429171085357666\n",
      "110 5.407247066497803\n",
      "111 5.385771751403809\n",
      "112 5.364694118499756\n",
      "113 5.343987464904785\n",
      "114 5.32375431060791\n",
      "115 5.303927421569824\n",
      "116 5.28446102142334\n",
      "117 5.26535177230835\n",
      "118 5.246613502502441\n",
      "119 5.228200912475586\n",
      "120 5.210003852844238\n",
      "121 5.192081451416016\n",
      "122 5.174437999725342\n",
      "123 5.157053470611572\n",
      "124 5.139968395233154\n",
      "125 5.123109340667725\n",
      "126 5.106408596038818\n",
      "127 5.0898919105529785\n",
      "128 5.07361364364624\n",
      "129 5.057448387145996\n",
      "130 5.041561603546143\n",
      "131 5.025891304016113\n",
      "132 5.01035737991333\n",
      "133 4.995006561279297\n",
      "134 4.979875087738037\n",
      "135 4.964930534362793\n",
      "136 4.950192928314209\n",
      "137 4.935637474060059\n",
      "138 4.9212493896484375\n",
      "139 4.907034873962402\n",
      "140 4.892936706542969\n",
      "141 4.879000663757324\n",
      "142 4.865217208862305\n",
      "143 4.851505279541016\n",
      "144 4.838003158569336\n",
      "145 4.824711322784424\n",
      "146 4.8115739822387695\n",
      "147 4.798549652099609\n",
      "148 4.785640239715576\n",
      "149 4.772799491882324\n",
      "150 4.760054111480713\n",
      "151 4.747758865356445\n",
      "152 4.7356486320495605\n",
      "153 4.723692417144775\n",
      "154 4.711678504943848\n",
      "155 4.699655055999756\n",
      "156 4.687777519226074\n",
      "157 4.676013946533203\n",
      "158 4.664431571960449\n",
      "159 4.6529316902160645\n",
      "160 4.641631126403809\n",
      "161 4.6303863525390625\n",
      "162 4.619228839874268\n",
      "163 4.608158588409424\n",
      "164 4.597212314605713\n",
      "165 4.586329460144043\n",
      "166 4.575544357299805\n",
      "167 4.564892768859863\n",
      "168 4.554455280303955\n",
      "169 4.54416036605835\n",
      "170 4.533994674682617\n",
      "171 4.523968696594238\n",
      "172 4.514046669006348\n",
      "173 4.504175662994385\n",
      "174 4.494384288787842\n",
      "175 4.484631538391113\n",
      "176 4.474940776824951\n",
      "177 4.4652323722839355\n",
      "178 4.455587863922119\n",
      "179 4.446059226989746\n",
      "180 4.436636447906494\n",
      "181 4.427343368530273\n",
      "182 4.417994499206543\n",
      "183 4.408775329589844\n",
      "184 4.399631977081299\n",
      "185 4.390554904937744\n",
      "186 4.3815741539001465\n",
      "187 4.372764587402344\n",
      "188 4.364057540893555\n",
      "189 4.355459213256836\n",
      "190 4.346985816955566\n",
      "191 4.338612079620361\n",
      "192 4.330287456512451\n",
      "193 4.322041034698486\n",
      "194 4.3137526512146\n",
      "195 4.305456161499023\n",
      "196 4.297245502471924\n",
      "197 4.2890191078186035\n",
      "198 4.280913829803467\n",
      "199 4.272885322570801\n",
      "200 4.264913082122803\n",
      "201 4.257022380828857\n",
      "202 4.249218940734863\n",
      "203 4.241469860076904\n",
      "204 4.233772277832031\n",
      "205 4.2261528968811035\n",
      "206 4.218642711639404\n",
      "207 4.211276531219482\n",
      "208 4.204025745391846\n",
      "209 4.196883678436279\n",
      "210 4.1898274421691895\n",
      "211 4.182803630828857\n",
      "212 4.175849914550781\n",
      "213 4.168937683105469\n",
      "214 4.162093162536621\n",
      "215 4.155359745025635\n",
      "216 4.148670673370361\n",
      "217 4.1420159339904785\n",
      "218 4.135395050048828\n",
      "219 4.12880802154541\n",
      "220 4.122286796569824\n",
      "221 4.115845203399658\n",
      "222 4.109468460083008\n",
      "223 4.103186130523682\n",
      "224 4.096986293792725\n",
      "225 4.090879440307617\n",
      "226 4.084805488586426\n",
      "227 4.078790664672852\n",
      "228 4.07281494140625\n",
      "229 4.066867828369141\n",
      "230 4.060962200164795\n",
      "231 4.055081367492676\n",
      "232 4.049276351928711\n",
      "233 4.043548107147217\n",
      "234 4.037827491760254\n",
      "235 4.032124042510986\n",
      "236 4.0264716148376465\n",
      "237 4.020877361297607\n",
      "238 4.015321254730225\n",
      "239 4.009819984436035\n",
      "240 4.004350185394287\n",
      "241 3.998882293701172\n",
      "242 3.993459701538086\n",
      "243 3.9881060123443604\n",
      "244 3.9827628135681152\n",
      "245 3.977449893951416\n",
      "246 3.972165107727051\n",
      "247 3.966923713684082\n",
      "248 3.9617249965667725\n",
      "249 3.956568717956543\n",
      "250 3.951449394226074\n",
      "251 3.946350336074829\n",
      "252 3.9412636756896973\n",
      "253 3.9362165927886963\n",
      "254 3.9312260150909424\n",
      "255 3.9262619018554688\n",
      "256 3.9213204383850098\n",
      "257 3.9163177013397217\n",
      "258 3.9113128185272217\n",
      "259 3.9063632488250732\n",
      "260 3.9014713764190674\n",
      "261 3.896587371826172\n",
      "262 3.891752243041992\n",
      "263 3.8869612216949463\n",
      "264 3.882277011871338\n",
      "265 3.877631187438965\n",
      "266 3.8730294704437256\n",
      "267 3.8684539794921875\n",
      "268 3.863889455795288\n",
      "269 3.859367847442627\n",
      "270 3.8548598289489746\n",
      "271 3.8503732681274414\n",
      "272 3.8459177017211914\n",
      "273 3.8414907455444336\n",
      "274 3.837080955505371\n",
      "275 3.8327267169952393\n",
      "276 3.8284051418304443\n",
      "277 3.824108123779297\n",
      "278 3.8198325634002686\n",
      "279 3.8155853748321533\n",
      "280 3.811361789703369\n",
      "281 3.8071653842926025\n",
      "282 3.802969455718994\n",
      "283 3.7987403869628906\n",
      "284 3.794541835784912\n",
      "285 3.7903571128845215\n",
      "286 3.786180019378662\n",
      "287 3.7820253372192383\n",
      "288 3.7779035568237305\n",
      "289 3.773794174194336\n",
      "290 3.76969575881958\n",
      "291 3.7656197547912598\n",
      "292 3.7615647315979004\n",
      "293 3.7575368881225586\n",
      "294 3.7535202503204346\n",
      "295 3.749520778656006\n",
      "296 3.745542526245117\n",
      "297 3.741586208343506\n",
      "298 3.73764705657959\n",
      "299 3.733736515045166\n",
      "300 3.729848623275757\n",
      "301 3.725999116897583\n",
      "302 3.7222251892089844\n",
      "303 3.718477725982666\n",
      "304 3.7147440910339355\n",
      "305 3.7110137939453125\n",
      "306 3.707296371459961\n",
      "307 3.7035903930664062\n",
      "308 3.6999142169952393\n",
      "309 3.69624662399292\n",
      "310 3.692598342895508\n",
      "311 3.6889610290527344\n",
      "312 3.6853199005126953\n",
      "313 3.6816914081573486\n",
      "314 3.678068161010742\n",
      "315 3.6744532585144043\n",
      "316 3.670868396759033\n",
      "317 3.6672964096069336\n",
      "318 3.663742780685425\n",
      "319 3.6602134704589844\n",
      "320 3.6566755771636963\n",
      "321 3.653153896331787\n",
      "322 3.6496620178222656\n",
      "323 3.6461727619171143\n",
      "324 3.6426961421966553\n",
      "325 3.6392478942871094\n",
      "326 3.6357834339141846\n",
      "327 3.6323227882385254\n",
      "328 3.628878116607666\n",
      "329 3.6254515647888184\n",
      "330 3.6220436096191406\n",
      "331 3.618638277053833\n",
      "332 3.6152353286743164\n",
      "333 3.6118569374084473\n",
      "334 3.6084835529327393\n",
      "335 3.6051108837127686\n",
      "336 3.6017680168151855\n",
      "337 3.598442554473877\n",
      "338 3.5951383113861084\n",
      "339 3.591848850250244\n",
      "340 3.5885567665100098\n",
      "341 3.585275650024414\n",
      "342 3.582014322280884\n",
      "343 3.578756809234619\n",
      "344 3.5755112171173096\n",
      "345 3.5722880363464355\n",
      "346 3.5690736770629883\n",
      "347 3.5658745765686035\n",
      "348 3.5626888275146484\n",
      "349 3.559504270553589\n",
      "350 3.5563392639160156\n",
      "351 3.553189277648926\n",
      "352 3.5500495433807373\n",
      "353 3.5469305515289307\n",
      "354 3.54382061958313\n",
      "355 3.5407228469848633\n",
      "356 3.537647247314453\n",
      "357 3.5345544815063477\n",
      "358 3.531477928161621\n",
      "359 3.528430223464966\n",
      "360 3.5253758430480957\n",
      "361 3.5223429203033447\n",
      "362 3.5193190574645996\n",
      "363 3.5162975788116455\n",
      "364 3.5132932662963867\n",
      "365 3.5103020668029785\n",
      "366 3.5073108673095703\n",
      "367 3.504330635070801\n",
      "368 3.5013625621795654\n",
      "369 3.4983930587768555\n",
      "370 3.4954450130462646\n",
      "371 3.4925005435943604\n",
      "372 3.4895706176757812\n",
      "373 3.486677646636963\n",
      "374 3.4837913513183594\n",
      "375 3.480907440185547\n",
      "376 3.478045701980591\n",
      "377 3.475184917449951\n",
      "378 3.4723362922668457\n",
      "379 3.4695374965667725\n",
      "380 3.4667320251464844\n",
      "381 3.4639320373535156\n",
      "382 3.461155414581299\n",
      "383 3.4583799839019775\n",
      "384 3.455601692199707\n",
      "385 3.452843427658081\n",
      "386 3.4500832557678223\n",
      "387 3.4473299980163574\n",
      "388 3.4445974826812744\n",
      "389 3.4418721199035645\n",
      "390 3.4391465187072754\n",
      "391 3.436448097229004\n",
      "392 3.4337525367736816\n",
      "393 3.4310646057128906\n",
      "394 3.428389549255371\n",
      "395 3.425722360610962\n",
      "396 3.4230520725250244\n",
      "397 3.42041015625\n",
      "398 3.4177801609039307\n",
      "399 3.4151411056518555\n",
      "400 3.412527322769165\n",
      "401 3.4099249839782715\n",
      "402 3.4073352813720703\n",
      "403 3.404775857925415\n",
      "404 3.402221918106079\n",
      "405 3.3996691703796387\n",
      "406 3.397141695022583\n",
      "407 3.3946194648742676\n",
      "408 3.3921003341674805\n",
      "409 3.3895974159240723\n",
      "410 3.3870933055877686\n",
      "411 3.3845858573913574\n",
      "412 3.3821020126342773\n",
      "413 3.379624843597412\n",
      "414 3.37716007232666\n",
      "415 3.3747024536132812\n",
      "416 3.372243881225586\n",
      "417 3.3697965145111084\n",
      "418 3.3673617839813232\n",
      "419 3.3649210929870605\n",
      "420 3.3624937534332275\n",
      "421 3.3600800037384033\n",
      "422 3.357664108276367\n",
      "423 3.355257511138916\n",
      "424 3.3528733253479004\n",
      "425 3.3504762649536133\n",
      "426 3.348088264465332\n",
      "427 3.3457226753234863\n",
      "428 3.3433468341827393\n",
      "429 3.3409817218780518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430 3.3386363983154297\n",
      "431 3.336292266845703\n",
      "432 3.333953380584717\n",
      "433 3.331620693206787\n",
      "434 3.3292765617370605\n",
      "435 3.32694411277771\n",
      "436 3.3246235847473145\n",
      "437 3.3223109245300293\n",
      "438 3.320004940032959\n",
      "439 3.3177084922790527\n",
      "440 3.315420150756836\n",
      "441 3.313142776489258\n",
      "442 3.310872793197632\n",
      "443 3.308605194091797\n",
      "444 3.306349992752075\n",
      "445 3.3041012287139893\n",
      "446 3.301853656768799\n",
      "447 3.2996232509613037\n",
      "448 3.2973949909210205\n",
      "449 3.2951607704162598\n",
      "450 3.292935609817505\n",
      "451 3.290714740753174\n",
      "452 3.2884888648986816\n",
      "453 3.2862679958343506\n",
      "454 3.284053325653076\n",
      "455 3.281841516494751\n",
      "456 3.2796311378479004\n",
      "457 3.277431011199951\n",
      "458 3.27522611618042\n",
      "459 3.273031234741211\n",
      "460 3.27085280418396\n",
      "461 3.2686657905578613\n",
      "462 3.266489028930664\n",
      "463 3.264322280883789\n",
      "464 3.262155055999756\n",
      "465 3.2599997520446777\n",
      "466 3.2578492164611816\n",
      "467 3.25569486618042\n",
      "468 3.253571033477783\n",
      "469 3.251431941986084\n",
      "470 3.249295234680176\n",
      "471 3.2471845149993896\n",
      "472 3.2450716495513916\n",
      "473 3.2429637908935547\n",
      "474 3.2408699989318848\n",
      "475 3.2387800216674805\n",
      "476 3.23669171333313\n",
      "477 3.234622001647949\n",
      "478 3.2325541973114014\n",
      "479 3.230485200881958\n",
      "480 3.2284317016601562\n",
      "481 3.226375102996826\n",
      "482 3.224327325820923\n",
      "483 3.2222867012023926\n",
      "484 3.220249652862549\n",
      "485 3.2182185649871826\n",
      "486 3.2161808013916016\n",
      "487 3.2141470909118652\n",
      "488 3.212118625640869\n",
      "489 3.21010422706604\n",
      "490 3.2080914974212646\n",
      "491 3.206085681915283\n",
      "492 3.204094409942627\n",
      "493 3.2020986080169678\n",
      "494 3.200103282928467\n",
      "495 3.1981236934661865\n",
      "496 3.1961545944213867\n",
      "497 3.19417667388916\n",
      "498 3.1922080516815186\n",
      "499 3.190247058868408\n"
     ]
    }
   ],
   "source": [
    "losses1 = []\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model1(X_train)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-mexican",
   "metadata": {},
   "source": [
    "Now let's try a new model with more neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cathedral-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Net(D_in, 1000, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "exposed-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "operational-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 132.76609802246094\n",
      "1 4904.0068359375\n",
      "2 131057.3125\n",
      "3 6154.6884765625\n",
      "4 2856.466552734375\n",
      "5 1520.4168701171875\n",
      "6 821.390380859375\n",
      "7 449.6463623046875\n",
      "8 250.7775421142578\n",
      "9 144.0603485107422\n",
      "10 86.67938995361328\n",
      "11 55.77344512939453\n",
      "12 39.09730529785156\n",
      "13 30.076574325561523\n",
      "14 25.176441192626953\n",
      "15 22.495473861694336\n",
      "16 21.010234832763672\n",
      "17 20.169645309448242\n",
      "18 19.67672348022461\n",
      "19 19.371776580810547\n",
      "20 19.168611526489258\n",
      "21 19.020845413208008\n",
      "22 18.903308868408203\n",
      "23 18.802547454833984\n",
      "24 18.71117401123047\n",
      "25 18.625205993652344\n",
      "26 18.54237174987793\n",
      "27 18.46154022216797\n",
      "28 18.382234573364258\n",
      "29 18.304237365722656\n",
      "30 18.22711753845215\n",
      "31 18.150726318359375\n",
      "32 18.075117111206055\n",
      "33 18.000028610229492\n",
      "34 17.925151824951172\n",
      "35 17.850730895996094\n",
      "36 17.77680778503418\n",
      "37 17.703367233276367\n",
      "38 17.630416870117188\n",
      "39 17.557992935180664\n",
      "40 17.48587989807129\n",
      "41 17.413753509521484\n",
      "42 17.34189224243164\n",
      "43 17.270355224609375\n",
      "44 17.198863983154297\n",
      "45 17.127490997314453\n",
      "46 17.05658531188965\n",
      "47 16.985788345336914\n",
      "48 16.915189743041992\n",
      "49 16.844602584838867\n",
      "50 16.77362823486328\n",
      "51 16.702367782592773\n",
      "52 16.63113021850586\n",
      "53 16.55961799621582\n",
      "54 16.486961364746094\n",
      "55 16.414310455322266\n",
      "56 16.341522216796875\n",
      "57 16.2678165435791\n",
      "58 16.1931209564209\n",
      "59 16.11808967590332\n",
      "60 16.04248046875\n",
      "61 15.966338157653809\n",
      "62 15.889946937561035\n",
      "63 15.812463760375977\n",
      "64 15.734234809875488\n",
      "65 15.655651092529297\n",
      "66 15.575309753417969\n",
      "67 15.493171691894531\n",
      "68 15.408656120300293\n",
      "69 15.321799278259277\n",
      "70 15.234670639038086\n",
      "71 15.146307945251465\n",
      "72 15.05586051940918\n",
      "73 14.9636812210083\n",
      "74 14.86967945098877\n",
      "75 14.772944450378418\n",
      "76 14.67184066772461\n",
      "77 14.568735122680664\n",
      "78 14.464533805847168\n",
      "79 14.357977867126465\n",
      "80 14.248998641967773\n",
      "81 14.13840389251709\n",
      "82 14.022863388061523\n",
      "83 13.905235290527344\n",
      "84 13.784917831420898\n",
      "85 13.660637855529785\n",
      "86 13.533267974853516\n",
      "87 13.4035062789917\n",
      "88 13.271677017211914\n",
      "89 13.138575553894043\n",
      "90 13.002057075500488\n",
      "91 12.863317489624023\n",
      "92 12.723186492919922\n",
      "93 12.583446502685547\n",
      "94 12.443805694580078\n",
      "95 12.304288864135742\n",
      "96 12.16307258605957\n",
      "97 12.020112037658691\n",
      "98 11.876225471496582\n",
      "99 11.732695579528809\n",
      "100 11.589991569519043\n",
      "101 11.447639465332031\n",
      "102 11.305516242980957\n",
      "103 11.163102149963379\n",
      "104 11.020727157592773\n",
      "105 10.879400253295898\n",
      "106 10.738445281982422\n",
      "107 10.598848342895508\n",
      "108 10.460759162902832\n",
      "109 10.323322296142578\n",
      "110 10.18735122680664\n",
      "111 10.052314758300781\n",
      "112 9.918441772460938\n",
      "113 9.785927772521973\n",
      "114 9.654884338378906\n",
      "115 9.52492618560791\n",
      "116 9.396732330322266\n",
      "117 9.270467758178711\n",
      "118 9.145862579345703\n",
      "119 9.022419929504395\n",
      "120 8.899765014648438\n",
      "121 8.778899192810059\n",
      "122 8.659523010253906\n",
      "123 8.541425704956055\n",
      "124 8.425610542297363\n",
      "125 8.3117036819458\n",
      "126 8.199419975280762\n",
      "127 8.088957786560059\n",
      "128 7.980584621429443\n",
      "129 7.873400688171387\n",
      "130 7.767900466918945\n",
      "131 7.663882255554199\n",
      "132 7.561713218688965\n",
      "133 7.462047100067139\n",
      "134 7.364055156707764\n",
      "135 7.2678937911987305\n",
      "136 7.174102783203125\n",
      "137 7.081660747528076\n",
      "138 6.991082668304443\n",
      "139 6.902939319610596\n",
      "140 6.81695556640625\n",
      "141 6.732667922973633\n",
      "142 6.649858474731445\n",
      "143 6.569008827209473\n",
      "144 6.490285396575928\n",
      "145 6.413039207458496\n",
      "146 6.337657928466797\n",
      "147 6.264386177062988\n",
      "148 6.193248271942139\n",
      "149 6.12463903427124\n",
      "150 6.058388710021973\n",
      "151 5.99412727355957\n",
      "152 5.9319233894348145\n",
      "153 5.871725082397461\n",
      "154 5.813251495361328\n",
      "155 5.756415843963623\n",
      "156 5.701672554016113\n",
      "157 5.648475170135498\n",
      "158 5.596889972686768\n",
      "159 5.546945571899414\n",
      "160 5.498419761657715\n",
      "161 5.451488494873047\n",
      "162 5.406075477600098\n",
      "163 5.362185955047607\n",
      "164 5.319794178009033\n",
      "165 5.278549671173096\n",
      "166 5.238703727722168\n",
      "167 5.200179576873779\n",
      "168 5.162853240966797\n",
      "169 5.126501083374023\n",
      "170 5.091414928436279\n",
      "171 5.057414531707764\n",
      "172 5.024550437927246\n",
      "173 4.992684364318848\n",
      "174 4.961911678314209\n",
      "175 4.932223320007324\n",
      "176 4.90361213684082\n",
      "177 4.876154899597168\n",
      "178 4.849746227264404\n",
      "179 4.824224948883057\n",
      "180 4.79954719543457\n",
      "181 4.775766849517822\n",
      "182 4.752562522888184\n",
      "183 4.729948997497559\n",
      "184 4.708070755004883\n",
      "185 4.686942100524902\n",
      "186 4.666590213775635\n",
      "187 4.646971225738525\n",
      "188 4.6278486251831055\n",
      "189 4.609320640563965\n",
      "190 4.591317176818848\n",
      "191 4.573829650878906\n",
      "192 4.5569376945495605\n",
      "193 4.540645599365234\n",
      "194 4.524877548217773\n",
      "195 4.509608268737793\n",
      "196 4.494835376739502\n",
      "197 4.480456829071045\n",
      "198 4.466509819030762\n",
      "199 4.452980041503906\n",
      "200 4.4398674964904785\n",
      "201 4.427132606506348\n",
      "202 4.41470193862915\n",
      "203 4.402591228485107\n",
      "204 4.3908185958862305\n",
      "205 4.379306793212891\n",
      "206 4.368105888366699\n",
      "207 4.357195854187012\n",
      "208 4.346621513366699\n",
      "209 4.33633279800415\n",
      "210 4.326296806335449\n",
      "211 4.316497802734375\n",
      "212 4.306957244873047\n",
      "213 4.297672748565674\n",
      "214 4.288638591766357\n",
      "215 4.279839515686035\n",
      "216 4.271209716796875\n",
      "217 4.26275634765625\n",
      "218 4.254448890686035\n",
      "219 4.246295928955078\n",
      "220 4.238300323486328\n",
      "221 4.230496883392334\n",
      "222 4.222833156585693\n",
      "223 4.215332508087158\n",
      "224 4.207995414733887\n",
      "225 4.200796604156494\n",
      "226 4.193722724914551\n",
      "227 4.186766624450684\n",
      "228 4.179936408996582\n",
      "229 4.173214435577393\n",
      "230 4.166598320007324\n",
      "231 4.1600661277771\n",
      "232 4.153632164001465\n",
      "233 4.147310256958008\n",
      "234 4.141117095947266\n",
      "235 4.1350507736206055\n",
      "236 4.129076957702637\n",
      "237 4.123186111450195\n",
      "238 4.1173787117004395\n",
      "239 4.111665725708008\n",
      "240 4.106019020080566\n",
      "241 4.100438594818115\n",
      "242 4.094934940338135\n",
      "243 4.089502334594727\n",
      "244 4.084103107452393\n",
      "245 4.078769683837891\n",
      "246 4.073505878448486\n",
      "247 4.068313121795654\n",
      "248 4.063174724578857\n",
      "249 4.0580549240112305\n",
      "250 4.052950382232666\n",
      "251 4.047900199890137\n",
      "252 4.042877197265625\n",
      "253 4.037901878356934\n",
      "254 4.032998085021973\n",
      "255 4.028132915496826\n",
      "256 4.023312091827393\n",
      "257 4.018537998199463\n",
      "258 4.013816833496094\n",
      "259 4.009138107299805\n",
      "260 4.004500865936279\n",
      "261 3.9999115467071533\n",
      "262 3.995361328125\n",
      "263 3.9908478260040283\n",
      "264 3.9863662719726562\n",
      "265 3.9819350242614746\n",
      "266 3.9775516986846924\n",
      "267 3.9732072353363037\n",
      "268 3.9689149856567383\n",
      "269 3.9646713733673096\n",
      "270 3.9604709148406982\n",
      "271 3.9563066959381104\n",
      "272 3.9521734714508057\n",
      "273 3.948075532913208\n",
      "274 3.944017171859741\n",
      "275 3.9399921894073486\n",
      "276 3.935992956161499\n",
      "277 3.932002067565918\n",
      "278 3.928034543991089\n",
      "279 3.924079418182373\n",
      "280 3.92014741897583\n",
      "281 3.9162144660949707\n",
      "282 3.912332534790039\n",
      "283 3.908475399017334\n",
      "284 3.904635190963745\n",
      "285 3.900808334350586\n",
      "286 3.897028684616089\n",
      "287 3.8932807445526123\n",
      "288 3.8895485401153564\n",
      "289 3.8858482837677\n",
      "290 3.882171154022217\n",
      "291 3.8785314559936523\n",
      "292 3.874918222427368\n",
      "293 3.8713247776031494\n",
      "294 3.8677539825439453\n",
      "295 3.8642079830169678\n",
      "296 3.8606865406036377\n",
      "297 3.8571839332580566\n",
      "298 3.8536972999572754\n",
      "299 3.8502144813537598\n",
      "300 3.8467512130737305\n",
      "301 3.8433165550231934\n",
      "302 3.8398935794830322\n",
      "303 3.836491346359253\n",
      "304 3.8331069946289062\n",
      "305 3.8297393321990967\n",
      "306 3.826390504837036\n",
      "307 3.82305908203125\n",
      "308 3.819734573364258\n",
      "309 3.8164169788360596\n",
      "310 3.8131208419799805\n",
      "311 3.8098511695861816\n",
      "312 3.8065974712371826\n",
      "313 3.8033530712127686\n",
      "314 3.800112009048462\n",
      "315 3.7968904972076416\n",
      "316 3.7936887741088867\n",
      "317 3.7904868125915527\n",
      "318 3.787287950515747\n",
      "319 3.7840933799743652\n",
      "320 3.780912160873413\n",
      "321 3.7777390480041504\n",
      "322 3.774571180343628\n",
      "323 3.7714264392852783\n",
      "324 3.7683024406433105\n",
      "325 3.7651991844177246\n",
      "326 3.762117624282837\n",
      "327 3.759052276611328\n",
      "328 3.755995273590088\n",
      "329 3.7529613971710205\n",
      "330 3.7499494552612305\n",
      "331 3.746952533721924\n",
      "332 3.7439613342285156\n",
      "333 3.7409887313842773\n",
      "334 3.738030433654785\n",
      "335 3.7350878715515137\n",
      "336 3.73215651512146\n",
      "337 3.7292373180389404\n",
      "338 3.7263336181640625\n",
      "339 3.7234396934509277\n",
      "340 3.720562696456909\n",
      "341 3.7176854610443115\n",
      "342 3.714817523956299\n",
      "343 3.711963653564453\n",
      "344 3.7091102600097656\n",
      "345 3.7062530517578125\n",
      "346 3.7034058570861816\n",
      "347 3.7005772590637207\n",
      "348 3.697758436203003\n",
      "349 3.6949524879455566\n",
      "350 3.692162275314331\n",
      "351 3.689384937286377\n",
      "352 3.6866261959075928\n",
      "353 3.6839144229888916\n",
      "354 3.6812360286712646\n",
      "355 3.678565263748169\n",
      "356 3.6759045124053955\n",
      "357 3.673248291015625\n",
      "358 3.670599937438965\n",
      "359 3.6679646968841553\n",
      "360 3.665337085723877\n",
      "361 3.662724494934082\n",
      "362 3.66011643409729\n",
      "363 3.657515287399292\n",
      "364 3.6549174785614014\n",
      "365 3.6523241996765137\n",
      "366 3.6497347354888916\n",
      "367 3.647153377532959\n",
      "368 3.644575595855713\n",
      "369 3.6420140266418457\n",
      "370 3.6394619941711426\n",
      "371 3.6369237899780273\n",
      "372 3.634389877319336\n",
      "373 3.6318745613098145\n",
      "374 3.6293725967407227\n",
      "375 3.626878261566162\n",
      "376 3.6243913173675537\n",
      "377 3.6219210624694824\n",
      "378 3.6194663047790527\n",
      "379 3.6170172691345215\n",
      "380 3.614577054977417\n",
      "381 3.6121389865875244\n",
      "382 3.6096973419189453\n",
      "383 3.6072592735290527\n",
      "384 3.6048338413238525\n",
      "385 3.602417469024658\n",
      "386 3.600008010864258\n",
      "387 3.597609281539917\n",
      "388 3.5952186584472656\n",
      "389 3.592839479446411\n",
      "390 3.5904643535614014\n",
      "391 3.5880985260009766\n",
      "392 3.5857410430908203\n",
      "393 3.583371639251709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 3.5810110569000244\n",
      "395 3.5786592960357666\n",
      "396 3.5763165950775146\n",
      "397 3.5739827156066895\n",
      "398 3.5716607570648193\n",
      "399 3.569352865219116\n",
      "400 3.5670506954193115\n",
      "401 3.5647592544555664\n",
      "402 3.5624656677246094\n",
      "403 3.5601792335510254\n",
      "404 3.557905673980713\n",
      "405 3.5556492805480957\n",
      "406 3.553406238555908\n",
      "407 3.5511701107025146\n",
      "408 3.548945188522339\n",
      "409 3.546731948852539\n",
      "410 3.5445258617401123\n",
      "411 3.542330741882324\n",
      "412 3.5401382446289062\n",
      "413 3.5379528999328613\n",
      "414 3.535778522491455\n",
      "415 3.5336124897003174\n",
      "416 3.531431198120117\n",
      "417 3.529247522354126\n",
      "418 3.527067184448242\n",
      "419 3.5249016284942627\n",
      "420 3.5227463245391846\n",
      "421 3.5206246376037598\n",
      "422 3.5185070037841797\n",
      "423 3.516395092010498\n",
      "424 3.514291286468506\n",
      "425 3.5121939182281494\n",
      "426 3.5101053714752197\n",
      "427 3.5080199241638184\n",
      "428 3.5059375762939453\n",
      "429 3.5038580894470215\n",
      "430 3.50178861618042\n",
      "431 3.499723434448242\n",
      "432 3.497662305831909\n",
      "433 3.4956088066101074\n",
      "434 3.4935622215270996\n",
      "435 3.4915175437927246\n",
      "436 3.4894776344299316\n",
      "437 3.4874415397644043\n",
      "438 3.485408306121826\n",
      "439 3.4833712577819824\n",
      "440 3.4813356399536133\n",
      "441 3.4793057441711426\n",
      "442 3.4772863388061523\n",
      "443 3.475278377532959\n",
      "444 3.473278045654297\n",
      "445 3.471285343170166\n",
      "446 3.469301462173462\n",
      "447 3.4673235416412354\n",
      "448 3.4653592109680176\n",
      "449 3.4634017944335938\n",
      "450 3.4614450931549072\n",
      "451 3.4594879150390625\n",
      "452 3.457538366317749\n",
      "453 3.4555962085723877\n",
      "454 3.453660249710083\n",
      "455 3.4517316818237305\n",
      "456 3.449815511703491\n",
      "457 3.447904586791992\n",
      "458 3.4460036754608154\n",
      "459 3.4441120624542236\n",
      "460 3.4422240257263184\n",
      "461 3.440343141555786\n",
      "462 3.4384665489196777\n",
      "463 3.436593532562256\n",
      "464 3.4347288608551025\n",
      "465 3.4328675270080566\n",
      "466 3.431011438369751\n",
      "467 3.4291539192199707\n",
      "468 3.4273018836975098\n",
      "469 3.425455093383789\n",
      "470 3.4236154556274414\n",
      "471 3.4217820167541504\n",
      "472 3.4199512004852295\n",
      "473 3.4181270599365234\n",
      "474 3.416308641433716\n",
      "475 3.414499282836914\n",
      "476 3.41269850730896\n",
      "477 3.410902261734009\n",
      "478 3.409111261367798\n",
      "479 3.40732741355896\n",
      "480 3.4055492877960205\n",
      "481 3.403778553009033\n",
      "482 3.4020137786865234\n",
      "483 3.4002528190612793\n",
      "484 3.3984994888305664\n",
      "485 3.3967509269714355\n",
      "486 3.3950095176696777\n",
      "487 3.3932743072509766\n",
      "488 3.3915436267852783\n",
      "489 3.389817714691162\n",
      "490 3.388096809387207\n",
      "491 3.386380910873413\n",
      "492 3.3846700191497803\n",
      "493 3.3829612731933594\n",
      "494 3.381258726119995\n",
      "495 3.379556655883789\n",
      "496 3.3778533935546875\n",
      "497 3.3761472702026367\n",
      "498 3.374439239501953\n",
      "499 3.3727331161499023\n"
     ]
    }
   ],
   "source": [
    "losses2 = []\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model2(X_train)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "    losses2.append(loss.item())\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "extra-consortium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe4e3756128>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses1, label=\"model1\")\n",
    "plt.plot(losses2, label=\"model2\")\n",
    "plt.ylim([0, 70])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-forestry",
   "metadata": {},
   "source": [
    "Let's compare the MSE loss on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "alpha-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# we need to normalize the test data with the min and max value\n",
    "# from the training data\n",
    "for col in numeric_data_test.columns:\n",
    "    numeric_data_test[col] = (numeric_data_test[col] - mins[col]) / (maxs[col] - mins[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "informal-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss for model1:  tensor(0.2574, grad_fn=<MseLossBackward>)\n",
      "MSE loss for model2:  tensor(0.3145, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame(numeric_data_test[\"SalePrice\"])\n",
    "y_test = torch.tensor(y_test_df.values, dtype=torch.float)\n",
    "x_test_df = numeric_data_test[numeric_x_columns]\n",
    "x_test = torch.tensor(x_test_df.values, dtype=torch.float)\n",
    "# prediction for model 1\n",
    "model1_pred = model1(x_test)\n",
    "print(\"MSE loss for model1: \", criterion(model1_pred, y_test))\n",
    "# prediction for model 2\n",
    "model2_pred = model2(x_test)\n",
    "print(\"MSE loss for model2: \", criterion(model2_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-rolling",
   "metadata": {},
   "source": [
    "## Now it is your turn!\n",
    "### Exercises\n",
    "\n",
    "1- Let's get back to model1. This time try to train it with a new optimizer. Try the Adam optimizer (which has shown to be faster than SGD for non-convex functions) and compare the trainig loss curve with SGD. Plot the training loss for the model trained with SGD and Adam optimizer.\n",
    "\n",
    "Note1: Use `torch.optim.Adam(model1.parameters(), lr=...)`\n",
    "\n",
    "Note2: If you are interested, check [this nice post](https://ruder.io/optimizing-gradient-descent/index.html) on differen gradient descent optimization algorithms.\n",
    "\n",
    "2- This time we want to build a new model with a new architecture. Specifically, we want to train a network with 3 hidden layers on the data. You can use the following code to build the architecture. Use the values 500, 1000, 200 for H1, H2, and H3 respectively. Train this new network on the same training data and compare it with the model1 we built above.\n",
    "\n",
    "```\n",
    "class Net_new(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super(Net_new, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(y_pred).clamp(min=0)\n",
    "        y_pred = self.linear3(y_pred).clamp(min=0)\n",
    "        y_pred = self.linear4(y_pred)\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-telescope",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
